# 短视频智能插入算力广告技术方案与执行计划

## 📋 项目概述

### 项目目标
在已有的短视频中智能插入NVIDIA算力相关的数字人软广告，通过AI技术实现自然、流畅的广告植入，提升用户观看体验和广告转化率。

### 核心特性
- 🎯 智能识别最佳广告插入点（基于文案分析）
- 🎭 数字人软广告（使用原视频人物形象和声音）
- 🔊 声音克隆技术（保持说话人音色一致）
- 🖼️ 图像清洗（去除字幕、水印等干扰元素）
- 🎬 无缝视频合成（自然过渡效果）

---

## 🏗️ 技术架构设计

### 整体架构图

```
┌─────────────────────────────────────────────────────────────┐
│                      主控制器 (Orchestrator)                  │
│                    coordinator.py                            │
└────────┬────────────────────────────────────────────────┬───┘
         │                                                 │
    ┌────▼─────────────────────────────────────┐     ┌───▼──────────────┐
    │        视频预处理模块                     │     │   广告生成模块    │
    │      (Video Processor)                   │     │  (Ad Generator)  │
    │  - 视频解析                               │     │  - 关键帧提取     │
    │  - 音频提取                               │     │  - 图片清洗       │
    │  - ASR语音识别                            │     │  - 声音克隆       │
    │  - 插入点智能选择                         │     │  - 数字人视频生成 │
    └───────────────────────────────────────────┘     └──────────────────┘
                       │                                       │
                       │                                       │
    ┌──────────────────▼───────────────────────────────────────▼─────┐
    │                    视频合成模块                                 │
    │                 (Video Compositor)                             │
    │  - 视频切割                                                     │
    │  - 过渡效果添加（淡入淡出）                                     │
    │  - 音视频同步                                                   │
    │  - 最终视频输出                                                 │
    └────────────────────────────────────────────────────────────────┘
                                  │
                                  ▼
                          最终视频输出
```

### 外部服务依赖

```
┌──────────────────────────────────────────────────────┐
│                   外部服务 API                        │
├──────────────────────────────────────────────────────┤
│  1. OpenAI API          → LLM判断 & 广告词生成        │
│  2. ComfyUI API         → 图片清洗/声音克隆/数字人     │
│     (103.231.86.148:9000)                           │
│  3. Whisper (本地)      → 语音识别 (ASR)              │
└──────────────────────────────────────────────────────┘
```

---

## 🧩 核心模块详细设计

### 1. 视频预处理模块 (video_processor.py)

#### 功能职责
- 视频元数据提取（分辨率、帧率、时长）
- 音频轨道提取
- ASR语音识别（文案提取）
- 智能插入点选择

#### 关键类设计

```python
class VideoProcessor:
    def __init__(self, video_path: str):
        """初始化视频处理器"""

    def extract_metadata(self) -> VideoMetadata:
        """提取视频元数据"""

    def extract_audio(self, output_path: str) -> str:
        """提取音频轨道"""

    def transcribe_audio(self, audio_path: str) -> TranscriptionResult:
        """语音识别，返回带时间戳的文案"""

    def find_insertion_point(self, transcription: TranscriptionResult) -> InsertionPoint:
        """智能选择广告插入点"""
```

#### 插入点选择策略

1. **基于LLM的文案分析**
   - 将完整文案发送给LLM
   - 要求LLM找出：
     - 自然的句子间隔点
     - 话题转折点
     - 适合插入广告的位置（按优先级排序）

2. **时间戳映射**
   - 将LLM识别的文字位置映射回视频时间
   - 使用Whisper的word-level时间戳

3. **安全区域过滤**
   - 避免开头3秒（黄金吸引期）
   - 避免最后5秒（结尾/CTA区域）
   - 确保插入点前后有足够时长（至少2秒）

---

### 2. 广告生成模块 (ad_generator.py)

#### 功能职责
- 提取插入点的关键帧（人物图像）
- 提取人声片段（用于声音克隆）
- 调用ComfyUI API生成广告素材

#### 关键类设计

```python
class AdGenerator:
    def __init__(self, comfyui_base_url: str, api_key: str):
        """初始化广告生成器"""

    def extract_keyframe(self, video_path: str, timestamp: float) -> np.ndarray:
        """提取指定时间点的视频帧"""

    def clean_image(self, image: np.ndarray, prompt: str = "去除字幕、水印、文字") -> np.ndarray:
        """调用ComfyUI清洗图片"""

    def extract_voice_sample(self, audio_path: str, start: float, duration: float = 5.0) -> str:
        """提取人声片段用于声音克隆"""

    def clone_voice(self, reference_audio: str, ad_script: str) -> str:
        """调用ComfyUI进行声音克隆"""

    def generate_digital_human_video(self, image: np.ndarray, audio: str) -> str:
        """生成数字人说话视频"""
```

#### ComfyUI API调用封装

```python
class ComfyUIClient:
    def __init__(self, base_url: str, api_key: str):
        """初始化ComfyUI客户端"""

    def run_workflow(self, workflow_id: str, inputs: dict) -> dict:
        """运行指定的workflow"""

    def get_result(self, task_id: str) -> bytes:
        """获取任务结果"""

    # 三个专用方法
    def image_edit(self, image: bytes, prompt: str) -> bytes:
        """调用Qwen Image Edit workflow"""

    def voice_clone(self, reference_audio: bytes, text: str) -> bytes:
        """调用IndexTTS2 workflow"""

    def digital_human(self, image: bytes, audio: bytes) -> bytes:
        """调用InfiniteTalk workflow"""
```

---

### 3. LLM服务模块 (llm_service.py)

#### 功能职责
- 判断视频是否适合插入广告
- 分析最佳插入点
- 生成广告词

#### 关键类设计

```python
class LLMService:
    def __init__(self, api_key: str, base_url: str, model: str):
        """初始化LLM服务"""

    def analyze_content(self, transcription: str) -> ContentAnalysis:
        """分析内容，判断是否适合插广告"""

    def find_insertion_points(self, transcription: str) -> List[InsertionSuggestion]:
        """找出最佳插入点（带时间戳和理由）"""

    def generate_ad_script(self,
                          context_before: str,
                          context_after: str,
                          video_theme: str = "",
                          product: str = "NVIDIA算力") -> str:
        """
        基于上下文生成广告词

        Args:
            context_before: 插入点之前的2-3句话
            context_after: 插入点之后的1-2句话
            video_theme: 视频主题/类型（可选）
            product: 广告产品（默认NVIDIA算力）

        Returns:
            str: 生成的广告词（15-20字）
        """
```

#### Prompt设计示例

```python
INSERTION_POINT_PROMPT = """
你是一个短视频广告植入专家。请分析以下视频文案，找出3个最适合插入广告的位置。

文案（带时间戳）：
{transcription_with_timestamps}

要求：
1. 找出自然的句子间隔点或话题转折点
2. 避免在视频开头3秒和结尾5秒
3. 优先选择有停顿或情绪转折的地方
4. 返回JSON格式，包含：时间戳、对应文字、选择理由、优先级(1-3)

返回格式：
[
  {
    "timestamp": 12.5,
    "text": "...前面的文字...",
    "reason": "这里是一个自然的话题转折点",
    "priority": 1
  }
]
"""

AD_SCRIPT_PROMPT = """
你是一个广告文案专家。你的任务是生成一段能够自然融入视频上下文的NVIDIA算力广告词。

## 视频上下文

**插入点之前的内容（上文）**：
{context_before}

**【广告词插入位置】**

**插入点之后的内容（下文）**：
{context_after}

## 视频主题
{video_theme}

## 要求
1. **自然衔接**：广告词必须能够无缝连接上文和下文，就像是原视频内容的一部分
2. **语气一致**：模仿原视频的语气、语速、风格（正式/轻松/专业等）
3. **话题相关**：基于上下文的话题，找到合理的切入点植入NVIDIA算力
4. **长度控制**：15-20个汉字，对应3-5秒的说话时长
5. **价值传递**：突出NVIDIA算力的核心优势（高性能、AI加速、训练提速等）

## 示例

**上文**："这个AI模型的训练速度非常快"
**下文**："接下来我们看看它的效果如何"
**生成**："这得益于NVIDIA强大的算力支持，让训练事半功倍"

**上文**："最近深度学习越来越火了"
**下文**："很多公司都在布局AI"
**生成**："想要在AI领域领先，NVIDIA GPU已成为标配算力"

## 输出格式
只返回广告词本身，不要包含引号、说明或其他内容。
"""
```

---

### 4. 视频合成模块 (video_compositor.py)

#### 功能职责
- 切割原视频
- 添加过渡效果
- 合成最终视频

#### 关键类设计

```python
class VideoCompositor:
    def __init__(self, output_dir: str):
        """初始化视频合成器"""

    def split_video(self, video_path: str, split_time: float) -> Tuple[str, str]:
        """在指定时间点切割视频"""

    def add_transition(self, video_path: str, fade_duration: float = 0.5) -> str:
        """添加淡入淡出效果"""

    def concatenate_videos(self, video_parts: List[str], output_path: str) -> str:
        """拼接多个视频片段"""

    def compose_final_video(self,
                          original_video: str,
                          ad_video: str,
                          insertion_point: float,
                          output_path: str) -> str:
        """合成最终视频"""
```

#### 合成流程

```
1. 切割原视频
   ├─ part1.mp4 (0s → 插入点)
   └─ part2.mp4 (插入点 → 结束)

2. 处理广告视频
   ├─ 添加淡入效果 (前0.3秒)
   └─ 添加淡出效果 (后0.3秒)

3. 处理原视频片段
   ├─ part1: 添加淡出效果 (最后0.3秒)
   └─ part2: 添加淡入效果 (开头0.3秒)

4. 音频混合
   ├─ 原视频音量: 插入点前后0.5秒逐渐降低/恢复
   └─ 广告音频: 正常音量

5. 拼接
   part1 + ad_video + part2 → final_output.mp4
```

---

### 5. 主控制器 (coordinator.py)

#### 功能职责
- 协调各个模块的执行
- 错误处理和重试
- 进度跟踪

#### 主流程

```python
class AdInsertionCoordinator:
    def process_video(self, input_video_path: str) -> str:
        """
        完整的广告插入流程

        Returns:
            str: 输出视频路径
        """
        # 1. 视频预处理
        processor = VideoProcessor(input_video_path)
        metadata = processor.extract_metadata()
        audio_path = processor.extract_audio()

        # 2. ASR语音识别
        transcription = processor.transcribe_audio(audio_path)

        # 3. LLM分析
        llm = LLMService()
        content_analysis = llm.analyze_content(transcription.text)

        if not content_analysis.suitable_for_ad:
            logger.info("视频内容不适合插入广告")
            return None

        # 4. 找插入点
        insertion_points = llm.find_insertion_points(transcription.text_with_timestamps)
        best_point = insertion_points[0]  # 最高优先级（只插入1个广告）

        # 5. 生成广告词（基于上下文）
        context_before = transcription.get_sentences_before(best_point.timestamp, count=3)
        context_after = transcription.get_sentences_after(best_point.timestamp, count=2)

        ad_script = llm.generate_ad_script(
            context_before=context_before,
            context_after=context_after,
            video_theme=content_analysis.theme  # 从LLM分析结果获取
        )

        # 6. 生成广告素材
        ad_gen = AdGenerator()

        # 6.1 提取关键帧
        keyframe = ad_gen.extract_keyframe(input_video_path, best_point.timestamp - 1)

        # 6.2 清洗图片
        clean_image = ad_gen.clean_image(keyframe)

        # 6.3 提取人声样本
        voice_sample = ad_gen.extract_voice_sample(
            audio_path,
            start=max(0, best_point.timestamp - 10),
            duration=5
        )

        # 6.4 声音克隆
        ad_audio = ad_gen.clone_voice(voice_sample, ad_script)

        # 6.5 生成数字人视频
        ad_video = ad_gen.generate_digital_human_video(clean_image, ad_audio)

        # 7. 视频合成
        compositor = VideoCompositor()
        final_video = compositor.compose_final_video(
            original_video=input_video_path,
            ad_video=ad_video,
            insertion_point=best_point.timestamp,
            output_path=self._generate_output_path(input_video_path)
        )

        return final_video
```

---

## 🛠️ 技术选型

### Python库依赖

| 库名 | 版本 | 用途 |
|------|------|------|
| `moviepy` | 1.0.3 | 视频处理（切割、合成、音频混合） |
| `openai-whisper` | latest | 语音识别（ASR） |
| `opencv-python` | 4.8+ | 图像处理、关键帧提取 |
| `openai` | 1.0+ | LLM API调用 |
| `requests` | 2.31+ | ComfyUI API调用 |
| `pydantic` | 2.0+ | 数据模型验证 |
| `python-dotenv` | 1.0+ | 环境变量管理 |
| `loguru` | 0.7+ | 日志管理 |
| `tqdm` | 4.66+ | 进度条显示 |
| `pillow` | 10.0+ | 图像处理辅助 |

### 可选依赖（进阶功能）

| 库名 | 用途 |
|------|------|
| `demucs` | 人声分离（提高声音克隆质量） |
| `scenedetect` | 场景切换检测（非文案类视频） |
| `mtcnn` | 人脸检测 |

---

## 📁 项目目录结构设计

```
中插广告/
├── README.md                       # 项目说明
├── 技术方案与执行计划.md            # 本文档
├── 需求文档.md                     # 需求文档
├── .env                            # 环境变量配置
├── .gitignore                      # Git忽略文件
├── requirements.txt                # Python依赖
│
├── input/                          # 输入视频目录
│   ├── 2025-09-23-DO803bmkdTl.mp4
│   ├── 2025-10-11-DPoue7CkhUX.mp4
│   └── 2025-10-22-DQFGqaeAS0u.mp4
│
├── output/                         # 输出视频目录
│   ├── processed/                  # 最终视频
│   ├── debug/                      # 调试中间结果
│   └── logs/                       # 处理日志
│
├── cache/                          # 缓存目录
│   ├── audio/                      # 提取的音频
│   ├── keyframes/                  # 提取的关键帧
│   ├── transcriptions/             # ASR结果缓存
│   └── ad_materials/               # 生成的广告素材
│
├── docs/                           # 文档目录
│   └── workflow/                   # ComfyUI workflow配置
│       ├── index TTS2情绪控制_api_1013.json
│       ├── InfiniteTalk数字人视频生视频_api.json
│       └── qwen_image_edit.json
│
├── src/                            # 源代码目录
│   ├── __init__.py
│   │
│   ├── core/                       # 核心模块
│   │   ├── __init__.py
│   │   ├── video_processor.py      # 视频预处理
│   │   ├── ad_generator.py         # 广告生成
│   │   ├── video_compositor.py     # 视频合成
│   │   └── coordinator.py          # 主控制器
│   │
│   ├── services/                   # 外部服务
│   │   ├── __init__.py
│   │   ├── llm_service.py          # LLM服务
│   │   ├── comfyui_client.py       # ComfyUI API客户端
│   │   └── asr_service.py          # 语音识别服务
│   │
│   ├── models/                     # 数据模型
│   │   ├── __init__.py
│   │   ├── video_models.py         # 视频相关数据模型
│   │   └── ad_models.py            # 广告相关数据模型
│   │
│   ├── utils/                      # 工具函数
│   │   ├── __init__.py
│   │   ├── file_utils.py           # 文件操作
│   │   ├── video_utils.py          # 视频工具
│   │   ├── audio_utils.py          # 音频工具
│   │   └── logger.py               # 日志配置
│   │
│   └── config/                     # 配置模块
│       ├── __init__.py
│       ├── settings.py             # 配置管理
│       └── prompts.py              # LLM提示词模板
│
├── scripts/                        # 脚本目录
│   ├── batch_process.py            # 批量处理脚本
│   ├── test_single_video.py        # 单视频测试
│   └── validate_workflows.py       # 验证ComfyUI workflows
│
├── tests/                          # 测试目录
│   ├── __init__.py
│   ├── test_video_processor.py
│   ├── test_ad_generator.py
│   └── test_compositor.py
│
└── main.py                         # 主入口程序
```

---

## 🔌 ComfyUI API接口设计

### RunningHub API封装

基于您提供的.env配置：

```python
# 配置
COMFYUI_BASE_URL = "http://103.231.86.148:9000"
API_KEYS = {
    "tts": "97f34a75960340febb0d0d614f3031a2",  # 专用于TTS
    "general": ["97f34a75960340febb0d0d614f3031a2", "3dd599e7ee9344b78d83417e5cc236ef"]  # 负载均衡
}

# Workflow IDs (需要从RunningHub获取)
WORKFLOWS = {
    "image_edit": "qwen_image_edit_workflow_id",
    "voice_clone": "index_tts2_workflow_id",
    "digital_human": "infinitetalk_workflow_id"
}
```

### API调用示例

```python
class RunningHubClient:
    def __init__(self):
        self.base_url = COMFYUI_BASE_URL
        self.api_keys = API_KEYS

    def call_workflow(self, workflow_type: str, inputs: dict) -> str:
        """
        调用ComfyUI workflow

        Args:
            workflow_type: "image_edit" | "voice_clone" | "digital_human"
            inputs: workflow输入参数

        Returns:
            str: 结果文件路径或URL
        """
        # 1. 上传输入文件（如图片、音频）
        # 2. 提交workflow任务
        # 3. 轮询任务状态
        # 4. 下载结果
        pass
```

---

## 📊 数据模型设计

### 核心数据结构

```python
from pydantic import BaseModel
from typing import List, Optional

class VideoMetadata(BaseModel):
    """视频元数据"""
    width: int
    height: int
    fps: float
    duration: float
    codec: str
    audio_codec: str

class TranscriptionSegment(BaseModel):
    """语音识别片段"""
    start: float
    end: float
    text: str

class TranscriptionResult(BaseModel):
    """完整语音识别结果"""
    text: str
    segments: List[TranscriptionSegment]
    language: str

    def get_sentences_before(self, timestamp: float, count: int = 3) -> str:
        """获取指定时间点之前的N个句子"""
        pass

    def get_sentences_after(self, timestamp: float, count: int = 2) -> str:
        """获取指定时间点之后的N个句子"""
        pass

class ContentAnalysis(BaseModel):
    """内容分析结果"""
    suitable_for_ad: bool
    reason: str
    theme: str  # 视频主题（如：科技、教育、创作等）

class InsertionPoint(BaseModel):
    """广告插入点"""
    timestamp: float
    text_context: str
    reason: str
    priority: int  # 1-3

class AdMaterial(BaseModel):
    """广告素材"""
    script: str  # 广告词
    audio_path: str  # 克隆后的音频
    video_path: str  # 数字人视频
    duration: float

class ProcessingResult(BaseModel):
    """处理结果"""
    success: bool
    input_video: str
    output_video: Optional[str]
    insertion_point: Optional[float]
    ad_script: Optional[str]
    error_message: Optional[str]
    processing_time: float
```

---

## 🚀 开发计划（分阶段实施）

### Phase 1: 基础框架搭建（第1周）

#### 目标
建立项目基础结构，实现基本的视频处理和API调用功能

#### 任务清单
- [x] 创建项目目录结构
- [ ] 配置环境（requirements.txt, .gitignore）
- [ ] 实现配置管理模块（config/settings.py）
- [ ] 实现日志系统（utils/logger.py）
- [ ] 实现基础的视频处理功能
  - [ ] 视频元数据提取
  - [ ] 音频提取
  - [ ] 关键帧提取
- [ ] 实现ComfyUI API客户端基础框架
- [ ] 编写单元测试

#### 验收标准
- 能够成功提取视频元数据和音频
- 能够提取指定时间点的视频帧
- ComfyUI API客户端能够成功连接

---

### Phase 2: ASR & LLM集成（第2周）

#### 目标
实现语音识别和LLM智能分析功能

#### 任务清单
- [ ] 集成Whisper进行ASR
  - [ ] 安装whisper模型
  - [ ] 实现带时间戳的文字转写
  - [ ] 实现结果缓存机制
- [ ] 实现LLM服务
  - [ ] 编写Prompt模板
  - [ ] 实现内容分析接口
  - [ ] 实现插入点选择接口
  - [ ] 实现广告词生成接口
- [ ] 实现插入点智能选择逻辑
  - [ ] 文案分析
  - [ ] 时间戳映射
  - [ ] 安全区域过滤

#### 验收标准
- Whisper能够准确转写视频文案（带时间戳）
- LLM能够准确分析并返回3个候选插入点
- 能够生成自然的广告词

---

### Phase 3: ComfyUI工作流集成（第3周）

#### 目标
完成三个ComfyUI workflow的API调用

#### 任务清单
- [ ] 调研RunningHub API文档
  - [ ] 文件上传接口
  - [ ] 任务提交接口
  - [ ] 结果查询接口
- [ ] 实现图片清洗workflow调用
  - [ ] 上传图片
  - [ ] 提交Qwen Image Edit任务
  - [ ] 下载清洗后的图片
- [ ] 实现声音克隆workflow调用
  - [ ] 上传参考音频
  - [ ] 提交IndexTTS2任务
  - [ ] 下载克隆后的音频
- [ ] 实现数字人视频生成workflow调用
  - [ ] 上传图片和音频
  - [ ] 提交InfiniteTalk任务
  - [ ] 下载数字人视频
- [ ] 实现任务队列和重试机制
- [ ] 优化API调用性能（并发控制、超时处理）

#### 验收标准
- 三个workflow都能成功调用
- 生成的数字人视频质量符合要求
- API调用稳定可靠（成功率>95%）

---

### Phase 4: 视频合成与优化（第4周）

#### 目标
实现视频合成和过渡效果

#### 任务清单
- [ ] 实现视频切割功能
- [ ] 实现过渡效果
  - [ ] 音频淡入淡出
  - [ ] 视频淡入淡出（可选）
  - [ ] 音量调节
- [ ] 实现视频拼接
- [ ] 音视频同步优化
- [ ] 输出视频质量优化
  - [ ] 保持原视频分辨率和帧率
  - [ ] 编码参数优化（H.264）
- [ ] 实现主控制器（coordinator.py）
  - [ ] 完整流程编排
  - [ ] 错误处理
  - [ ] 进度跟踪

#### 验收标准
- 最终视频播放流畅，无卡顿
- 广告插入自然，过渡平滑
- 音频音量一致，无突变

---

### Phase 5: 测试与优化（第5周）

#### 目标
全流程测试和性能优化

#### 任务清单
- [ ] 使用input目录的3个视频进行测试
- [ ] 修复发现的bug
- [ ] 性能优化
  - [ ] 缓存机制优化
  - [ ] 并发处理优化
  - [ ] 内存使用优化
- [ ] 边界情况处理
  - [ ] 无人声视频
  - [ ] 短视频（<10秒）
  - [ ] 无清晰人脸的视频
- [ ] 编写用户文档
- [ ] 实现批处理脚本

#### 验收标准
- 3个测试视频都能成功处理
- 单个视频处理时间 < 10分钟
- 处理成功率 > 90%

---

## ⚠️ 风险点与解决方案

### 风险1: ComfyUI API稳定性
**风险描述**: RunningHub服务可能不稳定，导致任务失败

**解决方案**:
- 实现自动重试机制（最多3次）
- 实现任务队列，失败的任务自动重新加入队列
- 添加超时控制（单个workflow超时10分钟）
- 记录失败日志，便于排查

---

### 风险2: 声音克隆质量不佳
**风险描述**: 如果原视频人声不清晰，克隆效果会很差

**解决方案**:
- 实现人声质量预检测
  - 音量检测
  - 背景噪音检测
- 如果质量不佳，采用备选方案：
  - 使用通用配音
  - 或提示用户手动选择更好的人声片段

---

### 风险3: 插入点选择不准确
**风险描述**: LLM可能选择不自然的插入点

**解决方案**:
- LLM返回3个候选点，按优先级排序
- 实现人工review模式（可选）
- 支持手动指定插入点
- 收集用户反馈，优化Prompt

---

### 风险4: 视频处理性能问题
**风险描述**: 大视频文件处理慢，内存占用高

**解决方案**:
- 视频分段处理
- 使用ffmpeg硬件加速
- 临时文件及时清理
- 限制同时处理的视频数量

---

### 风险5: Whisper识别准确率
**风险描述**: 对于方言、口音重的视频识别不准

**解决方案**:
- 使用large-v3模型（最高精度）
- 支持手动校正ASR结果
- 如果识别质量过低，标记为"不适合插广告"

---

## 🧪 测试计划

### 单元测试

| 模块 | 测试内容 |
|------|---------|
| video_processor | 元数据提取、音频提取、关键帧提取 |
| asr_service | 语音识别准确性、时间戳准确性 |
| llm_service | Prompt正确性、返回结果解析 |
| comfyui_client | API调用成功率、错误处理 |
| video_compositor | 视频切割、拼接、音视频同步 |

### 集成测试

| 场景 | 测试用例 |
|------|---------|
| 正常流程 | 使用标准短视频，验证完整流程 |
| 无人脸视频 | 验证fallback逻辑 |
| 短视频 | <10秒的视频，验证跳过逻辑 |
| 长视频 | >60秒的视频，验证性能 |
| 噪音视频 | 背景音乐很大的视频 |

### 验收测试

**测试视频**: input目录下的3个视频

**验收标准**:
- ✅ 能够成功识别文案
- ✅ 能够找到合理的插入点
- ✅ 生成的广告词自然
- ✅ 数字人视频质量良好（口型同步）
- ✅ 最终视频过渡自然
- ✅ 音频无突变

---

## 📝 广告词模板（NVIDIA算力）

```python
AD_TEMPLATES = {
    "科技类": [
        "NVIDIA算力加速，让AI触手可及",
        "强大算力支撑，尽在NVIDIA GPU",
        "NVIDIA算力引擎，AI时代的首选",
    ],
    "教育类": [
        "学习AI，从NVIDIA算力开始",
        "NVIDIA算力支持，让深度学习更简单",
    ],
    "创作类": [
        "创作提速，NVIDIA算力助力",
        "NVIDIA RTX，为创作者而生",
    ],
    "通用": [
        "NVIDIA算力，性能强劲",
        "选择NVIDIA，选择强大算力",
    ]
}
```

---

## 🎯 里程碑与交付物

### 第1周交付
- ✅ 项目结构搭建完成
- ✅ 基础视频处理功能可用
- ✅ 单元测试覆盖率 > 70%

### 第2周交付
- ✅ ASR功能正常工作
- ✅ LLM能够返回插入点建议
- ✅ 能够生成广告词

### 第3周交付
- ✅ 三个ComfyUI workflow都能调用
- ✅ 能够生成数字人视频
- ✅ API调用稳定

### 第4周交付
- ✅ 完整流程打通
- ✅ 能够输出最终视频
- ✅ 过渡效果自然

### 第5周交付（最终交付）
- ✅ 3个测试视频全部处理成功
- ✅ 批处理脚本可用
- ✅ 完整的使用文档
- ✅ 代码质量良好（有注释、有测试）

---

## 📚 相关文档

1. **RunningHub API文档**: 需要获取官方文档链接
2. **ComfyUI Workflow文档**: 已有三个JSON配置文件
3. **Whisper文档**: https://github.com/openai/whisper
4. **MoviePy文档**: https://zulko.github.io/moviepy/

---

## 🔄 后续优化方向（V2.0）

### 高级功能
1. **人声分离**: 使用Demucs提高声音克隆质量
2. **人脸检测**: 自动选择最佳关键帧
3. **场景检测**: 支持非文案类视频（才艺、风景等）
4. **多广告支持**: 根据内容匹配不同产品的广告
5. **A/B测试**: 生成多个版本，自动选择最佳

### 性能优化
1. **GPU加速**: 视频处理使用GPU
2. **并发处理**: 同时处理多个视频
3. **增量处理**: 缓存中间结果
4. **分布式**: 支持多机处理

### 用户体验
1. **Web界面**: 可视化操作界面
2. **实时预览**: 处理前预览插入效果
3. **手动调整**: 支持微调插入点和广告词
4. **批量模板**: 预设多种广告场景

---

## 🔍 方案审查与改进

### 审查概述

在初步方案基础上，进行了全面审查，发现并解决了多个关键问题。

---

### 已识别并解决的问题

#### ✅ P0级问题（已全部纳入方案）

##### 1. ComfyUI API调用方式 ✅ 已解决
- **原问题**: 不清楚如何调用ComfyUI API
- **解决方案**: 使用标准ComfyUI API（用户在103.231.86.148:9000运行ComfyUI实例）
- **详细说明**: 见 `docs/ComfyUI_API调用说明.md`

##### 2. 人声分离模块缺失 ✅ 已纳入Phase 1
- **问题**: 直接提取音频会包含背景音乐，影响声音克隆质量
- **解决方案**: 使用Demucs进行人声分离
- **实现位置**: Phase 1 - 基础框架

##### 3. 人脸检测和关键帧质量评估 ✅ 已纳入Phase 2
- **问题**: 直接提取固定时间点的帧可能无人脸/模糊/角度不佳
- **解决方案**: 提取多帧 → 人脸检测 → 质量评分 → 选择最佳帧
- **实现位置**: Phase 2 - ASR + 人脸检测

---

#### ⚠️ P1级问题（重要，逐步实现）

1. **音频质量检测**: 检测音量、信噪比、时长
2. **LLM时间戳映射精度**: Whisper是word-level，LLM是句子级别
3. **文件管理和清理策略**: 临时文件管理，避免磁盘占满
4. **性能估算**: 单视频约7-13分钟处理时间

---

#### 💡 P2级问题（建议改进）

1. **边界情况处理**: 无音轨、低分辨率、多人视频等
2. **色调统一**: 数字人视频与原视频色调匹配
3. **测试计划细化**: 准备多样化测试用例

---

### 修正后的核心流程

```
原视频输入
    ↓
[视频预处理]
    ├─ 提取元数据 ✓
    ├─ 提取音频 ✓
    └─ 基本检查（时长、分辨率、音轨）✓
    ↓
🆕 [人声分离] ← 新增（Demucs）
    └─ 分离人声和背景音
    ↓
[ASR语音识别]
    └─ Whisper转写（word-level时间戳）✓
    ↓
[LLM内容分析]
    ├─ 判断是否适合插广告 ✓
    ├─ 识别视频主题 ✓
    └─ 找出3个候选插入点（只选最佳1个）✓
    ↓
━━━━━━━━━━━━━━━━━━━━━━━━━━
    ↓
提取上下文（前2-3句 + 后1-2句）✓
    ↓
[LLM生成广告词]（基于上下文）✓
    ↓
━━━━━━━━━━━━━━━━━━━━━━━━━━
    ↓                        ↓
🆕 [多帧提取]           [提取人声片段]
    ↓                        ↓
🆕 [人脸检测]            🆕 [质量检测]
    ↓                        ↓
🆕 [质量评分]                ✓
    ↓                        ↓
[选最佳帧]                   ✓
    ↓                        ↓
[ComfyUI - 图片清洗] ←───────┘
    ↓                        ↓
    ↓              [ComfyUI - 声音克隆]
    ↓                        ↓
[ComfyUI - 数字人生成] ←──────┘
    ↓
🆕 [视频适配]（分辨率/帧率）
    ↓
🆕 [色调匹配]
    ↓
━━━━━━━━━━━━━━━━━━━━━━━━━━
    ↓
[视频合成]
    ├─ 切割 ✓
    ├─ 过渡效果 ✓
    └─ 拼接 ✓
    ↓
最终视频输出
```

**图例**: ✓ = 原方案 | 🆕 = 审查后新增

---

### 调整后的实施计划（5周）

#### **Phase 1: 基础框架 + 人声分离（第1周）**
- [ ] 创建项目目录结构
- [ ] 配置环境（requirements.txt, .env）
- [ ] 实现配置管理和日志系统
- [ ] **实现ComfyUI API客户端**（基于标准API）
- [ ] 测试三个workflow能否正常调用
- [ ] 实现视频处理基础功能
  - [ ] 视频元数据提取
  - [ ] 音频提取
  - [ ] 关键帧提取
- [ ] **实现人声分离模块（Demucs）** 🆕
- [ ] 实现文件管理器 🆕
- [ ] 编写单元测试

**验收**:
- ComfyUI API能成功调用三个workflow
- 能提取视频元数据、音频、关键帧
- 人声分离质量可用

---

#### **Phase 2: ASR + 人脸检测 + LLM（第2周）**
- [ ] 集成Whisper ASR
  - [ ] 带时间戳的转写
  - [ ] 结果缓存
- [ ] **实现人脸检测和质量评估** 🆕
  - [ ] 多帧提取
  - [ ] MTCNN人脸检测
  - [ ] 清晰度、角度、光线评分
- [ ] 实现LLM服务
  - [ ] 内容分析
  - [ ] 插入点选择（精确时间戳映射）
  - [ ] **上下文广告词生成** ✓
- [ ] 实现音频质量检测 🆕

**验收**:
- ASR准确率>90%
- 能找到最佳人脸关键帧
- LLM返回合理的插入点和上下文广告词

---

#### **Phase 3: ComfyUI工作流集成（第3-4周，2周）**
- [ ] 集成三个workflow
  - [ ] 图片清洗（Qwen Image Edit）
  - [ ] 声音克隆（IndexTTS2）
  - [ ] 数字人生成（InfiniteTalk）
- [ ] 调试和优化workflow参数
  - [ ] 节点ID映射
  - [ ] 输入参数适配
  - [ ] 输出文件获取
- [ ] **实现视频适配模块** 🆕
  - [ ] 分辨率匹配
  - [ ] 帧率转换
- [ ] 性能优化
  - [ ] 任务队列
  - [ ] 超时控制
  - [ ] 错误处理和重试

**验收**:
- 三个workflow稳定可用（成功率>95%）
- 数字人视频质量良好

---

#### **Phase 4: 视频合成与优化（第5周）**
- [ ] 实现视频切割
- [ ] 实现过渡效果
  - [ ] 音频淡入淡出
  - [ ] 视频淡入淡出
- [ ] **实现基本色调匹配** 🆕
- [ ] 实现视频拼接
- [ ] 实现主控制器
- [ ] 边界情况处理
- [ ] 错误恢复机制

**验收**:
- 完整流程打通
- 最终视频质量符合要求
- 处理成功率>90%

---

#### **Phase 5: 测试、优化、文档（第6周，可选）**
- [ ] 使用input目录的3个视频测试
- [ ] 修复bug
- [ ] 性能优化
- [ ] 补充边界情况处理
- [ ] 编写用户文档
- [ ] 实现批处理脚本

**验收**:
- 3个视频全部成功处理
- 文档完善

---

### 性能估算

**单视频处理时间**（30秒视频）:
```
1. 视频解析和音频提取: 10秒
2. 人声分离 (Demucs): 60-90秒  🆕
3. ASR (Whisper large-v3): 60-120秒
4. LLM分析和广告词生成: 10-20秒
5. 关键帧提取和评估: 10秒  🆕
6. ComfyUI - 图片清洗: 30-60秒
7. ComfyUI - 声音克隆: 60-120秒
8. ComfyUI - 数字人生成: 180-300秒
9. 视频合成: 30-60秒

总计: 约 7-13分钟/视频
```

**性能瓶颈**:
1. ComfyUI数字人生成（3-5分钟）
2. Whisper ASR（1-2分钟）
3. Demucs人声分离（1-1.5分钟）

---

### 风险评估

| 风险 | 级别 | 缓解措施 | 状态 |
|------|------|---------|------|
| ComfyUI API调用 | P0 | 使用标准API | ✅ 已解决 |
| 声音克隆质量差 | P0 | 人声分离 + 质量检测 | ✅ 已纳入方案 |
| 人脸质量差 | P0 | 多帧检测 + 质量评分 | ✅ 已纳入方案 |
| 处理时间过长 | P1 | 缓存 + 并发优化 | ⚠️ Phase 3优化 |
| 插入点不自然 | P1 | 上下文广告词生成 | ✅ 已实现 |
| 视频适配问题 | P1 | 分辨率/帧率转换 | ✅ 已纳入方案 |

---

### 硬件要求

**推荐配置**:
- CPU: 8核16线程
- 内存: 32GB
- GPU: NVIDIA RTX 3060或更好（12GB显存）
- 硬盘: 500GB SSD

**最低配置**:
- CPU: 4核8线程
- 内存: 16GB
- GPU: NVIDIA GPU（至少6GB显存）
- 硬盘: 100GB可用空间

---

## ✅ 总结

本技术方案设计了一套完整的短视频智能插入广告系统，主要特点：

1. **智能化**: 使用LLM自动分析最佳插入点
2. **自然化**: 数字人软广告，声音克隆保持一致性
3. **模块化**: 清晰的模块划分，易于维护和扩展
4. **可靠性**: 完善的错误处理和重试机制
5. **可扩展**: 预留接口支持后续功能扩展

预计开发周期：**5周**
预计首个版本交付时间：**第4周末**

**关键改进** (v2.0):
- ✅ 增加人声分离（Demucs）
- ✅ 增加人脸检测和质量评估
- ✅ 上下文感知的广告词生成
- ✅ 视频适配和色调匹配
- ✅ ComfyUI标准API集成

---

**文档版本**: v2.0（已整合审查报告）
**创建日期**: 2025-11-14
**最后更新**: 2025-11-14
